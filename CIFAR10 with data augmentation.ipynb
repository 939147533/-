{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import time\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import d2lzh_pytorch as d2l\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = './Datasets/CIFAR10'\n",
    "all_images = torchvision.datasets.CIFAR10(train=True, root=data_dir, download=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    \"\"\"\n",
    "    构造残差块\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, use_1x1conv=False, stride=1):\n",
    "    \n",
    "        super(Residual, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=stride)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        if use_1x1conv:\n",
    "            self.conv3 = nn.Conv2d(in_channels, out_channels, kernel_size=1,stride=stride)\n",
    "        else:\n",
    "            self.conv3 = None\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "    def forward(self, X):\n",
    "        Y = F.relu(self.bn1(self.conv1(X)))\n",
    "        Y = self.bn2(self.conv2(Y))\n",
    "        if self.conv3:\n",
    "            X = self.conv3(X)\n",
    "        return F.relu(Y + X)   \n",
    "\n",
    "def resnet_block(in_channels, out_channels, num_residuals, first_block=False):\n",
    "    if first_block:\n",
    "        assert in_channels == out_channels\n",
    "    blk = []\n",
    "    for i in range(num_residuals):\n",
    "        if i == 0 and not first_block:\n",
    "            # 除第一个模块外的其他模块，在第一个残差块里将上一个模块的通道数翻倍，并将高和宽减半\n",
    "            blk.append(Residual(in_channels, out_channels, use_1x1conv=True, stride=2))\n",
    "        else:\n",
    "            blk.append(Residual(out_channels, out_channels))\n",
    "    return nn.Sequential(*blk)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resnet18(num_classes):\n",
    "    # 第一个卷积层\n",
    "    net = nn.Sequential(\n",
    "        nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3),\n",
    "        nn.BatchNorm2d(64),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "    )\n",
    "    # 依次加入所有残差块，一共四个模块，每个模块使用两个残差块\n",
    "    net.add_module('resnet_block1', resnet_block(64, 64, 2, first_block=True))\n",
    "    net.add_module('resnet_block2', resnet_block(64, 128, 2))\n",
    "    net.add_module('resnet_block3', resnet_block(128, 256, 2))\n",
    "    net.add_module('resnet_block4', resnet_block(256, 512, 2))\n",
    "    # 加入全局平均池化层\n",
    "    net.add_module('global_avg_pool', d2l.GlobalAvgPool2d()) # output shape (batch_size, 512, 1, 1)\n",
    "    # 加入全连接层\n",
    "    net.add_module('fc', nn.Sequential(d2l.FlattenLayer(),nn.Linear(512, num_classes)))\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_workers = 4\n",
    "flip_aug = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.R andomHorizontalFlip(),\n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    "no_aug = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    "def load_cifar10(is_train, augs, batch_size, root=data_dir):\n",
    "    dataset = torchvision.datasets.CIFAR10(root=root, train=is_train, transform=augs, download=False)\n",
    "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=is_train, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(train_iter, test_iter, net, loss, optimizer, device, num_epochs):\n",
    "    net = net.to(device)\n",
    "    print('training on ', device)\n",
    "    batch_count = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        train_l_sum, train_acc_sum, n, start_time = 0.0, 0.0, 0, time.time()\n",
    "        for X, y in train_iter:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            y_hat = net(X)\n",
    "            l = loss(y_hat, y)\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            train_l_sum += l.cpu().item()\n",
    "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().cpu().item()\n",
    "            n += y.shape[0]\n",
    "            batch_count += 1\n",
    "        test_acc = d2l.evaluate_accuracy(test_iter, net)\n",
    "        print('Epoch %d, loss %.4f, train acc %.3f, test acc %.3f, time %.1f sec'%\n",
    "             (epoch + 1, train_l_sum / batch_count, train_acc_sum / n, test_acc, time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_with_data_aug(train_augs, test_augs, lr=0.001):\n",
    "    batch_size = 256\n",
    "    net = resnet18(10)\n",
    "    optimizer = optim.Adam(net.parameters(), lr=lr)\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    train_iter = load_cifar10(True, train_augs, batch_size)\n",
    "    test_iter = load_cifar10(False, test_augs, batch_size)\n",
    "    train(train_iter, test_iter, net, loss, optimizer, device, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on  cuda\n",
      "Epoch 1, loss 1.3555, train acc 0.507, test acc 0.431, time 33.4 sec\n",
      "Epoch 2, loss 0.5052, train acc 0.641, test acc 0.583, time 32.9 sec\n",
      "Epoch 3, loss 0.2852, train acc 0.697, test acc 0.611, time 34.8 sec\n",
      "Epoch 4, loss 0.1899, train acc 0.735, test acc 0.653, time 36.7 sec\n",
      "Epoch 5, loss 0.1365, train acc 0.759, test acc 0.619, time 36.9 sec\n",
      "Epoch 6, loss 0.1035, train acc 0.783, test acc 0.727, time 33.8 sec\n",
      "Epoch 7, loss 0.0805, train acc 0.804, test acc 0.730, time 34.1 sec\n",
      "Epoch 8, loss 0.0647, train acc 0.819, test acc 0.704, time 33.8 sec\n",
      "Epoch 9, loss 0.0533, train acc 0.833, test acc 0.745, time 34.0 sec\n",
      "Epoch 10, loss 0.0433, train acc 0.850, test acc 0.744, time 33.9 sec\n"
     ]
    }
   ],
   "source": [
    "train_with_data_aug(flip_aug, no_aug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
