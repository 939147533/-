{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.utils.data as Data\n",
    "\n",
    "sys.path.append('..')\n",
    "import d2lzh_pytorch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data_from_file(file_path):\n",
    "    with open(file_path, 'r') as fr:\n",
    "        lines = fr.readlines()\n",
    "    raw_dataset = [st.split() for st in lines]\n",
    "    print('sentence: %d' % len(raw_dataset))\n",
    "    return raw_dataset\n",
    "\n",
    "def load_data_from_text(text):\n",
    "    text = 'We are about to study the idea of a computational process.Computational processes are abstract beings that inhabit computers.As they evolve processes manipulate other abstract things called data'\n",
    "    sts = text.split('.')\n",
    "    raw_dataset = [st.split() for st in sts]\n",
    "    return raw_dataset\n",
    "\n",
    "    \n",
    "def token_index(raw_dataset):\n",
    "    counter = collections.Counter([tk for st in raw_dataset for tk in st])\n",
    "    counter = dict(filter(lambda x: x[1] >=1, counter.items()))\n",
    "    idx_to_token = [tk for tk,_ in counter.items()]\n",
    "    token_to_idx = {tk: idx for idx,tk in enumerate(idx_to_token)}\n",
    "    dataset = [[token_to_idx[tk] for tk in st if tk in token_to_idx] for st in raw_dataset]\n",
    "    num_tokens = sum([len(st) for st in dataset])\n",
    "    print('tokens: % d' % num_tokens)\n",
    "    return dataset, counter, idx_to_token, token_to_idx, num_tokens\n",
    "\n",
    "def discard(dataset, counter, num_tokens, idx_to_token, prop=1e-4):\n",
    "    '''\n",
    "    idx: 单词的数字索引\n",
    "    prop: 超参数，默认为1e-4\n",
    "    random.uniform(0,1)会随机产生0-1之间的数，当f_wi远大于prop，\n",
    "    即该单词出现次数很高，它与总词数之比远大于prop，\n",
    "    此时 1 - math.sqrt(prop / f_wi)近乎为1，\n",
    "    不等式成立，返回True,该词被丢弃，反之返回False,保留。\n",
    "    '''\n",
    "    subsampled_dataset = []\n",
    "    for st in dataset:\n",
    "        subsampled_st = []\n",
    "        for tk in st:\n",
    "            f_wi = counter[idx_to_token[tk]] / num_tokens\n",
    "            if not random.uniform(0,1) < 1 - math.sqrt(prop / f_wi):\n",
    "                subsampled_st.append(tk)\n",
    "        subsampled_dataset.append(subsampled_st)\n",
    "    sub_num_tokens = sum([len(st) for st in subsampled_dataset])\n",
    "    return subsampled_dataset, sub_num_tokens\n",
    "\n",
    "def get_centers_and_contexts(dataset, max_window_size):\n",
    "    centers, contexts = [], []\n",
    "    for st in dataset:\n",
    "        if len(st) < 2:\n",
    "            continue\n",
    "        centers += st\n",
    "        for center_i in range(len(st)):\n",
    "            window_size = random.randint(1, max_window_size)\n",
    "            indices = list(range(max(0, center_i - window_size), min(len(st), center_i + window_size + 1)))\n",
    "            indices.remove(center_i) # 去掉中心词\n",
    "            contexts.append([st[idx] for idx in indices])\n",
    "    return centers, contexts\n",
    "\n",
    "def get_negative(all_contexts, sampling_weights, K):\n",
    "    all_negatives, neg_candidates, i = [], [], 0\n",
    "    population = list(range(len(sampling_weights)))\n",
    "    for contexts in all_contexts:\n",
    "        negatives = []\n",
    "        while len(negatives) < len(contexts) * K:\n",
    "            if i == len(neg_candidates):\n",
    "                i = 0\n",
    "                # 从population中按照权重sampling_weights随机选取k(100000)个索引\n",
    "                neg_candidates = random.choices(population, sampling_weights, k=int(1e5))\n",
    "\n",
    "            neg = neg_candidates[i]\n",
    "            i += 1\n",
    "            if neg not in set(contexts): # 噪声词不能是背景词\n",
    "                negatives.append(neg)\n",
    "        all_negatives.append(negatives)\n",
    "    return all_negatives\n",
    "\n",
    "def load_all_data(file_path):\n",
    "    raw_dataset = load_data_from_file(file_path)\n",
    "    dataset, counter, idx_to_token, token_to_idx, num_tokens = token_index(raw_dataset)\n",
    "    subsampled_dataset, sub_num_tokens = discard(dataset, counter, num_tokens, idx_to_token, prop=1e-4)\n",
    "    all_centers, all_contexts = get_centers_and_contexts(subsampled_dataset, 5)\n",
    "    sampling_weights = [(counter[w] / sub_num_tokens)**0.75 for w in idx_to_token]\n",
    "    all_negatives = get_negative(all_contexts, sampling_weights, 5)\n",
    "    return all_centers, all_contexts, all_negatives, idx_to_token, token_to_idx\n",
    "\n",
    "def load_all_data2(text):\n",
    "    raw_dataset = load_data_from_text(text)\n",
    "    dataset, counter, idx_to_token, token_to_idx, num_tokens = token_index(raw_dataset)\n",
    "    all_centers, all_contexts = get_centers_and_contexts(dataset, 5)\n",
    "    weights = [(counter[w] / num_tokens)**0.75 for w in idx_to_token]\n",
    "    all_negatives = get_negative(all_contexts, dataset, 5)\n",
    "    return all_centers, all_contexts, all_negatives, idx_to_token, token_to_idx\n",
    "\n",
    "def skip_gram(center, contexts_and_negatives, embed_v, embed_u):\n",
    "    v = embed_v(center)\n",
    "    u = embed_u(contexts_and_negatives)\n",
    "    pred = torch.bmm(v, u.permute(0, 2, 1)) \n",
    "    return pred\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, centers, contexts, negatives):\n",
    "        assert len(centers) == len(contexts) == len(negatives)\n",
    "        self.centers = centers\n",
    "        self.contexts = contexts\n",
    "        self.negatives = negatives\n",
    "    def __getitem__(self, index):\n",
    "        return (self.centers[index], self.contexts[index], self.negatives[index])\n",
    "    def __len__(self):\n",
    "        return len(self.centers)\n",
    "\n",
    "def batch_data(data):\n",
    "    \"\"\"\n",
    "    用作DataLoader的参数collate_fn\n",
    "    data: 长为batch_size的list，list中的每个元素都是Dataset类调用__getitem__得到结果\n",
    "    \"\"\"\n",
    "    max_len = max(len(c) + len(n) for _, c, n in data)\n",
    "    centers, contexts_negatives, masks, labels = [], [], [], []\n",
    "    for center, context, negative in data:\n",
    "        cur_len = len(context) + len(negative)\n",
    "        centers += [center]\n",
    "        contexts_negatives += [context + negative + [0] * (max_len - cur_len)]\n",
    "        masks += [[1]*cur_len + [0]*(max_len - cur_len)]\n",
    "        labels += [[1]*len(context) + [0]*(max_len - len(context))]\n",
    "    return (torch.tensor(centers).view(-1,1),\n",
    "           torch.tensor(contexts_negatives),\n",
    "           torch.tensor(masks), torch.tensor(labels))    \n",
    "\n",
    "class SigmoidBinaryCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SigmoidBinaryCrossEntropyLoss, self).__init__()\n",
    "    def forward(self, inputs, targets, mask=None):\n",
    "        '''\n",
    "        input：predict， Tensor shape of (batch_size, len)\n",
    "        target：truth label，  Tensor of the same shape as input\n",
    "        mask: 用于指定batch中参与损失函数计算的部分预测值和标签\n",
    "        当掩码为1时，相应位置的预测值和标签将参与损失函数的计算；当掩码为0时，不参与计算\n",
    "        '''\n",
    "        inputs, targets, mask = inputs.float(), targets.float(), mask.float()\n",
    "        \n",
    "        res = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\", weight=mask)\n",
    "        return res.mean(dim=1) * mask.shape[1] /mask.float().sum(dim=1)\n",
    "    \n",
    "def train(net, data_iter, loss, optimizer, num_epochs):\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(\"train on\", device)\n",
    "    net = net.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        start, l_sum, n = time.time(), 0.0, 0\n",
    "        for batch in data_iter:\n",
    "            center, context_negative, mask, label = [d.to(device) for d in batch]\n",
    "            pred = skip_gram(center, context_negative, net[0], net[1])\n",
    "            l = loss(pred.view(label.shape), label, mask).mean() # 一个batch的平均loss\n",
    "            optimizer.zero_grad() #梯度清零\n",
    "            l.backward() # 计算梯度\n",
    "            optimizer.step() # 权值更新\n",
    "            l_sum += l.cpu().item()\n",
    "            n += 1\n",
    "        print('epoch %d, loss %.2f, time %.2fs' % (epoch + 1, l_sum / n, time.time() - start))\n",
    "        \n",
    "def train2(net, data_iter, loss, optimizer, num_epochs):\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(\"train on\", device)\n",
    "    net = net.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        start, l_sum, n = time.time(), 0.0, 0\n",
    "        for batch in data_iter:\n",
    "            center, context_negative, mask, label = [d.to(device) for d in batch]\n",
    "            pred = skip_gram(center, context_negative, net[0], net[1])\n",
    "            l = loss(pred.view(label.shape), label, mask).mean() # 一个batch的平均loss\n",
    "            optimizer.zero_grad() #梯度清零\n",
    "            l.backward() # 计算梯度\n",
    "            optimizer.step() # 权值更新\n",
    "            l_sum += l.cpu().item()\n",
    "            n += 1\n",
    "        print('epoch %d, loss %.2f, time %.2fs' % (epoch + 1, l_sum / n, time.time() - start))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: 42068\n",
      "tokens:  887521\n",
      "train on cuda\n",
      "epoch 1, loss 1.97, time 17.64s\n",
      "epoch 2, loss 0.63, time 17.80s\n",
      "epoch 3, loss 0.45, time 17.82s\n",
      "epoch 4, loss 0.40, time 17.79s\n",
      "epoch 5, loss 0.37, time 17.82s\n",
      "epoch 6, loss 0.35, time 17.86s\n",
      "epoch 7, loss 0.34, time 17.90s\n",
      "epoch 8, loss 0.33, time 17.86s\n",
      "epoch 9, loss 0.32, time 17.79s\n",
      "epoch 10, loss 0.32, time 17.78s\n",
      "epoch 11, loss 0.31, time 17.83s\n",
      "epoch 12, loss 0.31, time 17.82s\n",
      "epoch 13, loss 0.30, time 17.85s\n",
      "epoch 14, loss 0.30, time 17.84s\n",
      "epoch 15, loss 0.30, time 17.63s\n",
      "epoch 16, loss 0.29, time 17.85s\n",
      "epoch 17, loss 0.29, time 17.83s\n",
      "epoch 18, loss 0.29, time 17.83s\n",
      "epoch 19, loss 0.29, time 17.80s\n",
      "epoch 20, loss 0.28, time 17.83s\n",
      "epoch 21, loss 0.28, time 17.87s\n",
      "epoch 22, loss 0.28, time 17.83s\n",
      "epoch 23, loss 0.28, time 17.82s\n",
      "epoch 24, loss 0.28, time 17.65s\n",
      "epoch 25, loss 0.28, time 18.01s\n",
      "epoch 26, loss 0.28, time 17.71s\n",
      "epoch 27, loss 0.28, time 17.93s\n",
      "epoch 28, loss 0.28, time 17.90s\n",
      "epoch 29, loss 0.27, time 17.91s\n",
      "epoch 30, loss 0.27, time 17.92s\n",
      "epoch 31, loss 0.27, time 17.89s\n",
      "epoch 32, loss 0.27, time 17.93s\n",
      "epoch 33, loss 0.27, time 17.69s\n",
      "epoch 34, loss 0.27, time 18.11s\n",
      "epoch 35, loss 0.27, time 17.73s\n",
      "epoch 36, loss 0.27, time 17.92s\n",
      "epoch 37, loss 0.27, time 17.89s\n",
      "epoch 38, loss 0.27, time 17.90s\n",
      "epoch 39, loss 0.27, time 17.90s\n",
      "epoch 40, loss 0.27, time 17.89s\n",
      "epoch 41, loss 0.27, time 17.90s\n",
      "epoch 42, loss 0.27, time 17.92s\n",
      "epoch 43, loss 0.27, time 17.91s\n",
      "epoch 44, loss 0.27, time 17.75s\n",
      "epoch 45, loss 0.27, time 17.91s\n",
      "epoch 46, loss 0.27, time 17.89s\n",
      "epoch 47, loss 0.27, time 17.88s\n",
      "epoch 48, loss 0.27, time 17.89s\n",
      "epoch 49, loss 0.27, time 17.96s\n",
      "epoch 50, loss 0.27, time 17.90s\n",
      "epoch 51, loss 0.27, time 17.90s\n",
      "epoch 52, loss 0.27, time 17.90s\n",
      "epoch 53, loss 0.27, time 17.70s\n",
      "epoch 54, loss 0.27, time 17.91s\n",
      "epoch 55, loss 0.26, time 17.94s\n",
      "epoch 56, loss 0.26, time 18.00s\n",
      "epoch 57, loss 0.26, time 17.90s\n",
      "epoch 58, loss 0.26, time 17.90s\n",
      "epoch 59, loss 0.26, time 17.90s\n",
      "epoch 60, loss 0.26, time 17.88s\n",
      "epoch 61, loss 0.26, time 17.91s\n",
      "epoch 62, loss 0.26, time 17.73s\n",
      "epoch 63, loss 0.26, time 17.92s\n",
      "epoch 64, loss 0.26, time 17.91s\n",
      "epoch 65, loss 0.26, time 17.93s\n",
      "epoch 66, loss 0.26, time 17.91s\n",
      "epoch 67, loss 0.26, time 17.92s\n",
      "epoch 68, loss 0.26, time 17.95s\n",
      "epoch 69, loss 0.26, time 17.92s\n",
      "epoch 70, loss 0.26, time 17.91s\n",
      "epoch 71, loss 0.26, time 17.71s\n",
      "epoch 72, loss 0.26, time 18.11s\n",
      "epoch 73, loss 0.26, time 17.73s\n",
      "epoch 74, loss 0.26, time 17.90s\n",
      "epoch 75, loss 0.26, time 17.93s\n",
      "epoch 76, loss 0.26, time 17.90s\n",
      "epoch 77, loss 0.26, time 17.95s\n",
      "epoch 78, loss 0.26, time 17.91s\n",
      "epoch 79, loss 0.26, time 17.90s\n",
      "epoch 80, loss 0.26, time 17.71s\n",
      "epoch 81, loss 0.26, time 18.13s\n",
      "epoch 82, loss 0.26, time 17.74s\n",
      "epoch 83, loss 0.26, time 17.96s\n",
      "epoch 84, loss 0.26, time 17.89s\n",
      "epoch 85, loss 0.26, time 17.89s\n",
      "epoch 86, loss 0.26, time 17.90s\n",
      "epoch 87, loss 0.26, time 17.92s\n",
      "epoch 88, loss 0.26, time 17.91s\n",
      "epoch 89, loss 0.26, time 17.97s\n",
      "epoch 90, loss 0.26, time 17.92s\n",
      "epoch 91, loss 0.26, time 17.73s\n",
      "epoch 92, loss 0.26, time 17.93s\n",
      "epoch 93, loss 0.26, time 17.91s\n",
      "epoch 94, loss 0.26, time 17.92s\n",
      "epoch 95, loss 0.26, time 17.96s\n",
      "epoch 96, loss 0.26, time 17.96s\n",
      "epoch 97, loss 0.26, time 17.93s\n",
      "epoch 98, loss 0.26, time 17.90s\n",
      "epoch 99, loss 0.26, time 17.95s\n",
      "epoch 100, loss 0.26, time 17.71s\n"
     ]
    }
   ],
   "source": [
    "batch_size = 512\n",
    "num_workers = 0\n",
    "lr = 0.01\n",
    "num_epochs = 100\n",
    "# 读取数据\n",
    "file_path = r'E:\\ly\\Code\\Jupyter\\Pytorch\\Dive-into-DL-Pytorch\\Datasets\\ptb\\ptb.train.txt'\n",
    "all_centers, all_contexts, all_negatives, idx_to_token, token_to_idx = load_all_data(file_path)\n",
    "dataset = MyDataset(all_centers, all_contexts, all_negatives)\n",
    "data_iter = Data.DataLoader(dataset, batch_size, shuffle=True, collate_fn=batch_data, num_workers=num_workers)\n",
    "# 定义模型\n",
    "embed_size = 100\n",
    "net = nn.Sequential(\n",
    "        nn.Embedding(num_embeddings=len(token_to_idx), embedding_dim=embed_size),\n",
    "        nn.Embedding(num_embeddings=len(token_to_idx), embedding_dim=embed_size)\n",
    ")\n",
    "# 定义损失函数\n",
    "loss = SigmoidBinaryCrossEntropyLoss()\n",
    "# 定义优化函数\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "# 进行训练\n",
    "train(net, data_iter, loss, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine sim=0.416: candy\n",
      "cosine sim=0.404: claimants\n",
      "cosine sim=0.395: performed\n",
      "cosine sim=0.387: consistent\n",
      "cosine sim=0.386: labeled\n",
      "cosine sim=0.384: borrowings\n",
      "cosine sim=0.383: per\n",
      "cosine sim=0.381: pilot\n",
      "cosine sim=0.380: laid\n",
      "cosine sim=0.378: threats\n"
     ]
    }
   ],
   "source": [
    "def get_similar_tokens(query_token, k, embed):\n",
    "    W = embed.weight.data\n",
    "    x = W[token_to_idx[query_token]]\n",
    "    cos = torch.matmul(W, x) / (torch.sum(W * W, dim=1)\n",
    "                                * torch.sum(x * x) + 1e-9).sqrt()\n",
    "    _, topk = torch.topk(cos, k=k+1)\n",
    "    topk = topk.cpu().numpy()\n",
    "    for i in topk[1:]:\n",
    "        print('cosine sim=%.3f: %s' % (cos[i], (idx_to_token[i])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine sim=0.449: microsoft\n",
      "cosine sim=0.435: hurdles\n",
      "cosine sim=0.432: micro\n",
      "cosine sim=0.407: retailer\n",
      "cosine sim=0.406: mimic\n",
      "cosine sim=0.403: users\n",
      "cosine sim=0.397: intel\n",
      "cosine sim=0.393: fast-growing\n",
      "cosine sim=0.393: heights\n",
      "cosine sim=0.382: radar\n"
     ]
    }
   ],
   "source": [
    "get_similar_tokens('chip', 10, net[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine sim=0.416: candy\n",
      "cosine sim=0.404: claimants\n",
      "cosine sim=0.395: performed\n",
      "cosine sim=0.387: consistent\n",
      "cosine sim=0.386: labeled\n",
      "cosine sim=0.384: borrowings\n",
      "cosine sim=0.383: per\n",
      "cosine sim=0.381: pilot\n",
      "cosine sim=0.380: laid\n",
      "cosine sim=0.378: threats\n"
     ]
    }
   ],
   "source": [
    "get_similar_tokens('asbestos', 10, net[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100])\n",
      "torch.Size([9999, 100])\n",
      "torch.Size([9999])\n",
      "[5779 8932 9617 4615 5972 2661 6173 3049 3751 6307]\n",
      "8932 divisive\n",
      "9617 micro\n",
      "4615 hired\n",
      "5972 affecting\n",
      "2661 lying\n",
      "6173 fleet\n",
      "3049 adding\n",
      "3751 microwave\n",
      "6307 honduras\n"
     ]
    }
   ],
   "source": [
    "W1 = net[0].weight.data\n",
    "W2 = net[1].weight.data\n",
    "x = W1[token_to_idx['asbestos']]\n",
    "print(x.shape)\n",
    "print(W2.shape)\n",
    "pred = torch.matmul(x,W2.view(W2.shape[1], -1))\n",
    "print(pred.shape)\n",
    "\n",
    "_, topk = torch.topk(pred, 10)\n",
    "topk = topk.cpu().numpy()\n",
    "print(topk)\n",
    "\n",
    "for i in topk[1:]:\n",
    "    print(i,idx_to_token[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 处理数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence: 42068\n"
     ]
    }
   ],
   "source": [
    "with open(r'E:\\ly\\Code\\Jupyter\\Pytorch\\Dive-into-DL-Pytorch\\Datasets\\ptb\\ptb.train.txt', 'r') as fr:\n",
    "    lines = fr.readlines()\n",
    "    raw_dataset = [st.split() for st in lines]\n",
    "print('sentence: %d' % len(raw_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 建立词语索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counter = collections.Counter([tk for st in raw_dataset for tk in st])\n",
    "# 只保留在数据集中出现5次及以上的单词\n",
    "counter = dict(filter(lambda x: x[1] >= 5, counter.items()))\n",
    "# 将词映射到整数索引\n",
    "idx_to_token = [tk for tk,_ in counter.items()]\n",
    "token_to_idx = {tk: idx for idx,tk in enumerate(idx_to_token)}\n",
    "dataset = [[token_to_idx[tk] for tk in st if tk in token_to_idx] for st in raw_dataset]\n",
    "num_tokens = sum([len(st) for st in dataset])\n",
    "print('tokens: % d' % num_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 二次采样\n",
    "-  文本中一般会出现一些高频词，如\"the\",'a'等，这些词的高频出现对模型不利，\n",
    "- 因此进行二次采样来丢弃这些高频词, 每个单词被丢弃的概率为：\n",
    "- P(wi) = max(1-sqrt(t/f(wi), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discard(idx, prop=1e-4):\n",
    "    '''\n",
    "    idx: 单词的数字索引\n",
    "    prop: 超参数，默认为1e-4\n",
    "    random.uniform(0,1)会随机产生0-1之间的数，当f_wi远大于prop，\n",
    "    即该单词出现次数很高，它与总词数之比远大于prop，\n",
    "    此时 1 - math.sqrt(prop / f_wi)近乎为1，\n",
    "    不等式成立，返回True,该词被丢弃，反之返回False,保留。\n",
    "    '''\n",
    "    f_wi  = counter[idx_to_token[idx]] / num_tokens\n",
    "    return random.uniform(0,1) < 1 - math.sqrt(prop / f_wi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subsampled_dataset = [[tk for tk in st if not discard(tk)] for st in dataset]\n",
    "sub_num_tokens = sum([len(st) for st in subsampled_dataset])\n",
    "print('subsampled_tokens: %d' % sub_num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 比较一个词在二次采样前后出现在数据集中的次数\n",
    "def compare_counts(token):\n",
    "    num_before = sum([st.count(token_to_idx[token]) for st in dataset])\n",
    "    num_after = sum([st.count(token_to_idx[token]) for st in subsampled_dataset])\n",
    "    return '%s: before=%d, after=%d' % (token, num_before, num_after)\n",
    "\n",
    "compare_counts('the')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 提取中心词和背景词\n",
    "在1-max_window_size之间随机选择一个整数作为窗口半径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_centers_and_contexts(dataset, max_window_size):\n",
    "    centers, contexts = [], []\n",
    "    for st in dataset:\n",
    "        if len(st) < 2:\n",
    "            continue\n",
    "        centers += st\n",
    "        for center_i in range(len(st)):\n",
    "            window_size = random.randint(1, max_window_size)\n",
    "            indices = list(range(max(0, center_i - window_size), min(len(st), center_i + window_size + 1)))\n",
    "            indices.remove(center_i) # 去掉中心词\n",
    "            contexts.append([st[idx] for idx in indices])\n",
    "    return centers, contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_centers, all_contexts = get_centers_and_contexts(subsampled_dataset, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### negative sampling 负采样\n",
    "- 对于一对中心词和背景词，随机采样K个噪声词\n",
    "- 噪声词采样率P(w)设为w词频与总词频之比的0.75次方"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_negative(all_contexts, sampling_weights, K):\n",
    "    all_negatives, neg_candidates, i = [], [], 0\n",
    "    population = list(range(len(sampling_weights)))\n",
    "    for contexts in all_contexts:\n",
    "        negatives = []\n",
    "        while len(negatives) < len(contexts) * K:\n",
    "            if i == len(neg_candidates):\n",
    "                i = 0\n",
    "                # 从population中按照权重sampling_weights随机选取K个索引\n",
    "                neg_candidates = random.choices(population, sampling_weights, k=int(1e5))\n",
    "\n",
    "            neg = neg_candidates[i]\n",
    "            i += 1\n",
    "            if neg not in set(contexts): # 噪声词不能是背景词\n",
    "                negatives.append(neg)\n",
    "        all_negatives.append(negatives)\n",
    "    return all_negatives\n",
    "sampling_weights = [(counter[w] / sub_num_tokens)**0.75 for w in idx_to_token]\n",
    "all_negatives = get_negative(all_contexts, sampling_weights, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "#### 读取数据\n",
    "- 在一个batch中，每个样本都包括一个中心词、ni个背景词 + mi个噪声词,\"\n",
    "- contexts_negatives: 因为ni + mi不一样大，因此在后面填充0来统一为长度max(ni+mi)，\n",
    "- mask: contexts_negatives中为填充项时，mask中对应为0，其他为1\n",
    "- labels: contexts_negatives中为contexts时，labels中对应位置为1，其他为0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, centers, contexts, negatives):\n",
    "        assert len(centers) == len(contexts) == len(negatives)\n",
    "        self.centers = centers\n",
    "        self.contexts = contexts\n",
    "        self.negatives = negatives\n",
    "    def __getitem__(self, index):\n",
    "        return (self.centers[index], self.contexts[index], self.negatives[index])\n",
    "    def __len__(self):\n",
    "        return len(self.centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_data(data):\n",
    "    \"\"\"\n",
    "    用作DataLoader的参数collate_fn\n",
    "    data: 长为batch_size的list，list中的每个元素都是Dataset类调用__getitem__得到结果\n",
    "    \"\"\"\n",
    "    max_len = max(len(c) + len(n) for _, c, n in data)\n",
    "    centers, contexts_negatives, masks, labels = [], [], [], []\n",
    "    for center, context, negative in data:\n",
    "        cur_len = len(context) + len(negative)\n",
    "        centers += [center]\n",
    "        contexts_negatives += [context + negative + [0] * (max_len - cur_len)]\n",
    "        masks += [[1]*cur_len + [0]*(max_len - cur_len)]\n",
    "        labels += [[1]*len(context) + [0]*(max_len - len(context))]\n",
    "    return (torch.tensor(centers).view(-1,1),\n",
    "           torch.tensor(contexts_negatives),\n",
    "           torch.tensor(masks), torch.tensor(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip-gram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 小批量乘法\n",
    "- 使用小批量乘法运算bmm对两个小批量中的矩阵一一做乘法\n",
    "- (batch_size,1,embedding_dim) * (batch_size, max_len, embedding_dim) = (batch_size, 1, max_len)\n",
    "- torch.bmm(X,Y)\n",
    "- permut(将行列转置) (batch_size, max_len, embedding_dim) -> (batch_size, embedding_dim, max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Skip-gram Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def skip_gram(center, contexts_and_negatives, embed_v, embed_u):\n",
    "    v = embed_v(center)\n",
    "    u = embed_u(contexts_and_negatives)\n",
    "    pred = torch.bmm(v, u.permute(0, 2, 1)) \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 二元交叉熵损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SigmoidBinaryCrossEntropyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SigmoidBinaryCrossEntropyLoss, self).__init__()\n",
    "    def forward(self, inputs, targets, mask=None):\n",
    "        '''\n",
    "        input：predict， Tensor shape of (batch_size, len)\n",
    "        target：truth label，  Tensor of the same shape as input\n",
    "        mask: 用于指定batch中参与损失函数计算的部分预测值和标签\n",
    "        当掩码为1时，相应位置的预测值和标签将参与损失函数的计算；当掩码为0时，不参与计算\n",
    "        '''\n",
    "        inputs, targets, mask = inputs.float(), targets.float(), mask.float()\n",
    "        \n",
    "        res = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\", weight=mask)\n",
    "        return res.mean(dim=1) * mask.shape[1] /mask.float().sum(dim=1)\n",
    "    \n",
    "loss = SigmoidBinaryCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 初始化模型参数\n",
    "##### 嵌入层\n",
    "- 用来获取词向量的层，通过创建nn.Embedding实例得到\n",
    "- 嵌入层的权重是(num_embedding, embedding_dim)的矩阵，行数为词典大小，列数为每个词向量的维度\n",
    "- 输入：词的索引i\n",
    "- 输出：返回权重矩阵的第i行作为其词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed_size = 100\n",
    "net = nn.Sequential(\n",
    "        nn.Embedding(num_embeddings=len(idx_to_token), embedding_dim=embed_size),\n",
    "        nn.Embedding(num_embeddings=len(idx_to_token), embedding_dim=embed_size)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(net, lr, num_epochs):\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(\"train on\", device)\n",
    "    net = net.to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    for epoch in range(num_epochs):\n",
    "        start, l_sum, n = time.time(), 0.0, 0\n",
    "        for batch in data_iter:\n",
    "            center, context_negative, mask, label = [d.to(device) for d in batch]\n",
    "            pred = skip_gram(center, context_negative, net[0], net[1])\n",
    "            l = loss(pred.view(label.shape), label, mask).mean() # 一个batch的平均loss\n",
    "            optimizer.zero_grad() #梯度清零\n",
    "            l.backward() # 计算梯度\n",
    "            optimizer.step() # 权值更新\n",
    "            l_sum += l.cpu().item()\n",
    "            n += 1\n",
    "        print('epoch %d, loss %.2f, time %.2fs' % (epoch + 1, l_sum / n, time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "num_workers = 0\n",
    "dataset = MyDataset(all_centers, all_contexts, all_negatives)\n",
    "data_iter = Data.DataLoader(dataset, batch_size, shuffle=True, collate_fn=batch_data, num_workers=num_workers)\n",
    "train(net, 0.01, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 应用词嵌入模型\n",
    "- 根据两个词向量的余弦相似度表示词与词之间在语义上的相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_similar_tokens(query_token, k, embed):\n",
    "    W = embed.weight.data\n",
    "    x = W[token_to_idx[query_token]]\n",
    "    cos = torch.matmul(W, x) / (torch.sum(W * W, dim=1)\n",
    "                                * torch.sum(x * x) + 1e-9).sqrt()\n",
    "    _, topk = torch.topk(cos, k=k+1)\n",
    "    topk = topk.cpu().numpy()\n",
    "    for i in topk[1:]:\n",
    "        print('cosine sim=%.3f: %s' % (cos[i], (idx_to_token[i])))\n",
    "\n",
    "get_similar_tokens('government', 5, net[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 测试模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
